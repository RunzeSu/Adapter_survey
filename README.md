# A Deep Learning Survey for Adapters in Multitask Natural Language Understanding

In this github we summarized the most state-of-the-art adpaters architectures from 2019 to 2021.

## Implemented Algorithms


| Model | Algorithm | Year | CheckIn Status | 

|-------|---------|---------|---------|---------|

|  MTDNN  | Multi-Task Deep Neural Networks for Natural Language Understanding | 2019 |   ✅   |

|  BERT and PALs  | BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning | 2019 |   ✅   |

|  Adapter  | Parameter-efficient transfer learning for nlp | 2019 |   ✅   |

|  Embert  | Multitask Learning Using BERT with Task-Embedded Attention | 2021 |   ✅   |

|  Hyperformer  | Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks | 2021 |   ✅   |

|  conditional Adapter   | Conditionally Adaptive Multi-task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data | 2021 |   ✅   |



#### Datasets

We applied The General Language Understanding Evaluation (GLUE) benchmark as our evaluation dataset. For more details, please refer to the GLUE Official Website:

https://gluebenchmark.com/
