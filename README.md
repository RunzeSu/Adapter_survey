# A Deep Learning Survey for Adapters in Multitask Natural Language Understanding

In this github we summarized the most state-of-the-art adpaters architectures from 2019 to 2021.

## Implemented Algorithms


| Model | Algorithm | Year |



|  MTDNN  | Multi-Task Deep Neural Networks for Natural Language Understanding | 2019 

|  BERT and PALs  | BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning | 2019 

|  Adapter  | Parameter-efficient transfer learning for nlp | 2019 

|  Embert  | Multitask Learning Using BERT with Task-Embedded Attention | 2021 

|  Hyperformer  | Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks | 2021 

|  Conditional Adapter   | Conditionally Adaptive Multi-task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data | 2021 



### Datasets

We applied The General Language Understanding Evaluation (GLUE) benchmark as our evaluation dataset. For more details, please refer to the GLUE Official Website:

https://gluebenchmark.com/

The data can also be downloaded automatically by running:

```
python data/download_glue_data.py
```

### How to implement the models

##### MTDNN

#### BERT and PALs

#### Adapter

#### Embert

#### Hyperformer

#### Conditional Adapter 
